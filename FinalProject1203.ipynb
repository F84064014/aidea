{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip3 install sklearn_crfsuite","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting sklearn_crfsuite\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.7/site-packages (from sklearn_crfsuite) (4.45.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from sklearn_crfsuite) (0.8.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sklearn_crfsuite) (1.14.0)\nCollecting python-crfsuite>=0.8.3\n  Downloading python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n\u001b[K     |████████████████████████████████| 743 kB 6.2 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite\nSuccessfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport sklearn_crfsuite\nfrom sklearn_crfsuite import scorers\nfrom sklearn_crfsuite import metrics\nfrom sklearn_crfsuite.metrics import flat_classification_report","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def CRF(x_train, y_train, x_test, y_test):\n    crf = sklearn_crfsuite.CRF(\n        algorithm='lbfgs',\n        c1=0.1,\n        c2=0.1,\n        max_iterations=100,\n        all_possible_transitions=True\n    )\n    crf.fit(x_train, y_train)\n#     print(crf) #\n    y_pred = crf.predict(x_test)\n    y_pred_mar = crf.predict_marginals(x_test)\n\n#     print(y_pred_mar) #\n\n    labels = list(crf.classes_)\n    labels.remove('O')\n    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n    \n#     eli5.show_weights(crf, top=10, feature_re='^word\\.is',\n#                       horizontal_layout=False, show=['targets'])\n    \n    return y_pred, y_pred_mar, f1score","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pretrained word vectors\n# get a dict of tokens (key) and their pretrained word vectors (value)\n# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\ndim = 0\nword_vecs= {}\n# open pretrained word vector file\nwith open('../input/ai-course-final-project/cna.cbow.cwe_p.tar_g.512d.0.txt', encoding='utf-8') as f:\n    for line in f:\n        tokens = line.strip().split()\n\n        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n        if len(tokens) == 2:\n            dim = int(tokens[1])\n            continue\n    \n        word = tokens[0]\n        vec = np.array([ float(t) for t in tokens[1:] ])\n        word_vecs[word] = vec","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"vocabulary_size:  158566  word_vector_dim:  (512,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load `train.data` and separate into a list of labeled data of each text\n# return:\n#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n#   traindata_list: a list of lists, storing training data_list splitted from data_list\n#   testdata_list: a list of lists, storing testing data_list splitted from data_list\nfrom sklearn.model_selection import train_test_split\ndef Dataset(data_path):\n    with open(data_path, 'r', encoding='utf-8') as f:\n        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n    data_list, data_list_tmp = list(), list()\n    article_id_list=list()\n    idx=0\n    for row in data:\n        data_tuple = tuple()\n        if row == '\\n':\n            article_id_list.append(idx)\n            idx+=1\n            data_list.append(data_list_tmp)\n            data_list_tmp = []\n        else:\n            row = row.strip('\\n').split(' ')\n            data_tuple = (row[0], row[1])\n            data_list_tmp.append(data_tuple)\n    if len(data_list_tmp) != 0:\n        data_list.append(data_list_tmp)\n    \n    # here we random split data into training dataset and testing dataset\n    # but you should take `development data` or `test data` as testing data\n    # At that time, you could just delete this line, \n    # and generate data_list of `train data` and data_list of `development/test data` by this function\n    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n                                                                                                    article_id_list,\n                                                                                                    test_size=0.33,\n                                                                                                    random_state=42)\n    \n    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list ","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# open the pos tag file\ndf_POSTag = pd.read_csv('../input/aicoursefinalproject/POSTag.txt')\ndf_POSTag.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"  text_entity            POS  length\n0          醫師             Na       2\n1           ：  COLONCATEGORY       1\n2           你             Nh       1\n3           有              D       1\n4           做             VC       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_entity</th>\n      <th>POS</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>醫師</td>\n      <td>Na</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>：</td>\n      <td>COLONCATEGORY</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>你</td>\n      <td>Nh</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>有</td>\n      <td>D</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>做</td>\n      <td>VC</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(df_POSTag['text_entity'].unique()))\nprint(len(df_POSTag))\nprint(len(df_POSTag['POS'].unique()))","execution_count":9,"outputs":[{"output_type":"stream","text":"2244\n34416\n55\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_POSTag.drop_duplicates(inplace=True)\ndf_POSTag.head()\ntext_pool = set(df_POSTag['text_entity'])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look up the POSTag txt\n# encode the word\ndef POSTagEncode(data_list):\n    POSTag_list = list()\n    POSTag_label_code = list(df_POSTag['POS'].unique())\n    \n    for idx_list in range(len(data_list)):\n        POSTag_list_temp = list()\n        for idx_tuple in range(len(data_list[idx_list])):\n            key = data_list[idx_list][idx_tuple][0]\n            \n            pos_code_result_temp = list()\n            for idx_POSTag in range(len(df_POSTag)):\n                if key in df_POSTag['text_entity'].iloc[idx_POSTag]:\n                    for POS in POSTag_label_code:\n                        if POS == df_POSTag['POS'].iloc[idx_POSTag]:\n                            pos_code_result_temp.append(1)\n                        else:\n                            pos_code_result_temp.append(0)\n                    break\n            POSTag_list_temp.append(pos_code_result_temp)\n        POSTag_list.append(POSTag_list_temp)\n            \n    return POSTag_list","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look up word vectors\n# turn each word into its pretrained word vector\n# return a list of word vectors corresponding to each token in train.data\ndef Word2Vector(data_list, embedding_dict):\n    embedding_list = list()\n\n    # No Match Word (unknown word) Vector in Embedding\n    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n\n    for idx_list in range(len(data_list)):\n        embedding_list_tmp = list()\n        for idx_tuple in range(len(data_list[idx_list])):\n            key = data_list[idx_list][idx_tuple][0] # token\n\n            if key in embedding_dict:\n                value = embedding_dict[key]\n            else:\n                value = unk_vector\n            embedding_list_tmp.append(value)\n        embedding_list.append(embedding_list_tmp)\n    \n    return embedding_list","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input features: pretrained word vectors of each token\n# return a list of feature dicts, each feature dict corresponding to each token\ndef Feature(embed_list, p):\n    feature_list = list()\n    # feature of w2d (original)\n    for idx_list in range(len(embed_list)):\n        feature_list_tmp = list()\n        for idx_tuple in range(len(embed_list[idx_list])):\n            \n            feature_dict = dict()\n            # my feature\n            for idx_POS in range(len(list(df_POSTag['POS'].unique()))):\n                feature_dict[list(df_POSTag['POS'].unique())[idx_POS]] = p[idx_list][idx_tuple][idx_POS]\n            feature_list_tmp.append(feature_dict)\n            #-----------------\n            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n        feature_list.append(feature_list_tmp)              \n        \n    return feature_list","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the labels of each tokens in train.data\n# return a list of lists of labels\ndef Preprocess(data_list):\n    label_list = list()\n    for idx_list in range(len(data_list)):\n        label_list_tmp = list()\n        for idx_tuple in range(len(data_list[idx_list])):\n            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n        label_list.append(label_list_tmp)\n    return label_list","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset('../input/aicoursefinalproject/sample.data')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ptrain = POSTagEncode(traindata_list)\nptest  = POSTagEncode(testdata_list)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(ptrain))\nprint(len(ptrain[0]))\nprint(len(ptrain[0][0]))\n\nprint(len(trainembed_list))\nprint(len(trainembed_list[0]))\nprint(len(trainembed_list[0][0]))","execution_count":28,"outputs":[{"output_type":"stream","text":"17\n1759\n55\n17\n1759\n512\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Word Embedding\ntrainembed_list = Word2Vector(traindata_list, word_vecs)\ntestembed_list = Word2Vector(testdata_list, word_vecs)\n\n# CRF - Train Data (Augmentation Data)\nx_train = Feature(trainembed_list, ptrain)\ny_train = Preprocess(traindata_list)\n\n# CRF - Test Data (Golden Standard)\nx_test = Feature(testembed_list, ptest)\ny_test = Preprocess(testdata_list)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# each word with 521 dimension\nprint(x_train[0][1]['dim_1'])\nprint(len(x_train[0][1]))\nprint(y_train[0][0])","execution_count":39,"outputs":[{"output_type":"stream","text":"0.803237\n567\nO\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)","execution_count":40,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n  B-location      0.000     0.000     0.000        15\n  I-location      0.000     0.000     0.000        41\n  B-med_exam      0.125     0.030     0.049        33\n  I-med_exam      0.857     0.075     0.138        80\n     B-money      0.444     0.333     0.381        12\n     I-money      0.353     0.171     0.231        35\n      B-name      0.500     0.143     0.222         7\n      I-name      0.200     0.100     0.133        10\n      B-time      0.653     0.441     0.527       111\n      I-time      0.830     0.570     0.676       265\n\n   micro avg      0.709     0.360     0.477       609\n   macro avg      0.396     0.186     0.236       609\nweighted avg      0.638     0.360     0.436       609\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass labels=['B-location', 'I-location', 'B-med_exam', 'I-med_exam', 'B-money', 'I-money', 'B-name', 'I-name', 'B-time', 'I-time'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n  FutureWarning)\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1score","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"0.43629455120045096"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}