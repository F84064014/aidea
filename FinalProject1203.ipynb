{"cells":[{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.6.8\n"]}],"source":["!python --version"]},{"metadata":{"trusted":true},"cell_type":"code","source":["!pip3 install sklearn_crfsuite"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.6/dist-packages (0.3.6)\n","Requirement already satisfied: six in /home/warroom/.local/lib/python3.6/site-packages (from sklearn_crfsuite) (1.15.0)\n","Requirement already satisfied: tqdm>=2.0 in /home/warroom/.local/lib/python3.6/site-packages (from sklearn_crfsuite) (4.41.0)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.9.7)\n","Requirement already satisfied: tabulate in /home/warroom/.local/lib/python3.6/site-packages (from sklearn_crfsuite) (0.8.6)\n","\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"]}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","from sklearn_crfsuite.metrics import flat_classification_report"],"execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def CRF(x_train, y_train, x_test, y_test):\n","    crf = sklearn_crfsuite.CRF(\n","        algorithm='lbfgs',\n","        c1=0.1,\n","        c2=0.1,\n","        max_iterations=100,\n","        all_possible_transitions=True\n","    )\n","    crf.fit(x_train, y_train)\n","#     print(crf) #\n","    y_pred = crf.predict(x_test)\n","    y_pred_mar = crf.predict_marginals(x_test)\n","\n","#     print(y_pred_mar) #\n","\n","    labels = list(crf.classes_)\n","    labels.remove('O')\n","    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n","    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n","    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n","    \n","#     eli5.show_weights(crf, top=10, feature_re='^word\\.is',\n","#                       horizontal_layout=False, show=['targets'])\n","    \n","    return y_pred, y_pred_mar, f1score"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["import numpy as np"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# load pretrained word vectors\n","# get a dict of tokens (key) and their pretrained word vectors (value)\n","# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n","dim = 0\n","word_vecs= {}\n","# open pretrained word vector file\n","with open('Dataset/cna.cbow.cwe_p.tar_g.512d.0.txt', encoding='utf-8') as f:\n","    for line in f:\n","        tokens = line.strip().split()\n","\n","        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n","        if len(tokens) == 2:\n","            dim = int(tokens[1])\n","            continue\n","    \n","        word = tokens[0]\n","        vec = np.array([ float(t) for t in tokens[1:] ])\n","        word_vecs[word] = vec"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["vocabulary_size:  37230  word_vector_dim:  (353,)\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# load `train.data` and separate into a list of labeled data of each text\n","# return:\n","#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n","#   traindata_list: a list of lists, storing training data_list splitted from data_list\n","#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n","from sklearn.model_selection import train_test_split\n","def Dataset(data_path):\n","    with open(data_path, 'r', encoding='utf-8') as f:\n","        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n","    data_list, data_list_tmp = list(), list()\n","    article_id_list=list()\n","    idx=0\n","    for row in data:\n","        data_tuple = tuple()\n","        if row == '\\n':\n","            article_id_list.append(idx)\n","            idx+=1\n","            data_list.append(data_list_tmp)\n","            data_list_tmp = []\n","        else:\n","            row = row.strip('\\n').split(' ')\n","            data_tuple = (row[0], row[1])\n","            data_list_tmp.append(data_tuple)\n","    if len(data_list_tmp) != 0:\n","        data_list.append(data_list_tmp)\n","    \n","    # here we random split data into training dataset and testing dataset\n","    # but you should take `development data` or `test data` as testing data\n","    # At that time, you could just delete this line, \n","    # and generate data_list of `train data` and data_list of `development/test data` by this function\n","    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n","                                                                                                    article_id_list,\n","                                                                                                    test_size=0.33,\n","                                                                                                    random_state=42)\n","    \n","    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "],"execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# open the pos tag file\n","df_POSTag = pd.read_csv('Dataset/POSTag.txt')\n","df_POSTag.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   article_id text_entity            POS  length\n","0           1           醫             Na       2\n","1           1           師             Na       2\n","2           1           ：  COLONCATEGORY       1\n","3           1           你             Nh       1\n","4           1           有              D       1"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>text_entity</th>\n      <th>POS</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>醫</td>\n      <td>Na</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>師</td>\n      <td>Na</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>：</td>\n      <td>COLONCATEGORY</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>你</td>\n      <td>Nh</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>有</td>\n      <td>D</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":7}]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(len(df_POSTag['text_entity'].unique()))\n","print(len(df_POSTag))\n","print(len(df_POSTag['POS'].unique()))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1231\n47040\n55\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_POSTag.drop_duplicates(inplace=True)\n","df_POSTag.head()\n","text_pool = set(df_POSTag['text_entity'])"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# look up the POSTag txt\n","# encode the word\n","def POSTagEncode(data_list, article_id_list):\n","    \n","    POSTag_list = list()\n","\n","    df = pd.read_csv('Dataset/POSTag.txt')\n","    POSTag_label = list(df['POS'].unique())\n","    \n","    for idx_list in range(len(data_list)):\n","        df_temp = df[df['article_id']==article_id_list[idx_list]+1]\n","\n","        text_temp = list(df_temp['text_entity'])\n","        POS_temp  = list(df_temp['POS'])\n","        POSTag_list_temp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            word_POSTag_temp=list()\n","            temp=0\n","            for POSTag_label_code in POSTag_label:\n","                if POS_temp[idx_tuple] == POSTag_label_code:\n","                    word_POSTag_temp.append(temp)\n","                    break\n","                else:\n","                    temp+=1\n","            POSTag_list_temp.append(word_POSTag_temp)\n","        POSTag_list.append(POSTag_list_temp)\n","\n","    return POSTag_list"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# look up word vectors\n","# turn each word into its pretrained word vector\n","# return a list of word vectors corresponding to each token in train.data\n","def Word2Vector(data_list, embedding_dict):\n","    embedding_list = list()\n","\n","    # No Match Word (unknown word) Vector in Embedding\n","    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n","\n","    for idx_list in range(len(data_list)):\n","        embedding_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            key = data_list[idx_list][idx_tuple][0] # token\n","\n","            if key in embedding_dict:\n","                value = embedding_dict[key]\n","            else:\n","                value = unk_vector\n","            embedding_list_tmp.append(value)\n","        embedding_list.append(embedding_list_tmp)\n","    \n","    return embedding_list"],"execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# input features: pretrained word vectors of each token\n","# return a list of feature dicts, each feature dict corresponding to each token\n","def Feature(embed_list, p):\n","    \n","    df = pd.read_csv('Dataset/POSTag.txt')\n","    feature_list = list()\n","\n","    list_temp=list(df['POS'].unique())\n","\n","    # feature of w2d (original)\n","    for idx_list in range(len(embed_list)):\n","        feature_list_tmp = list()\n","        for idx_tuple in range(len(embed_list[idx_list])):\n","            \n","            feature_dict = dict()\n","            # my feature\n","            feature_dict[\"POS\"] = p[idx_list][idx_tuple][0]\n","            #print(f\"content with 0 {p[idx_list][idx_tuple][0]},content without 0 {p[idx_list][idx_tuple]}\")\n","            #-----------------\n","            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n","                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n","            feature_list_tmp.append(feature_dict)\n","        feature_list.append(feature_list_tmp)\n","        print(idx_list+1, '\\\\', len(embed_list)+1, ', # of token:', len(embed_list[idx_list]))\n","        \n","    return feature_list"],"execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# get the labels of each tokens in train.data\n","# return a list of lists of labels\n","def Preprocess(data_list):\n","    label_list = list()\n","    for idx_list in range(len(data_list)):\n","        label_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n","        label_list.append(label_list_tmp)\n","    return label_list"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset('Dataset/sample.data')"],"execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["ptrain = POSTagEncode(traindata_list, traindata_article_id_list)\n","ptest  = POSTagEncode(testdata_list, testdata_article_id_list)"],"execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"tags":[]},"cell_type":"code","source":["# Load Word Embedding\n","trainembed_list = Word2Vector(traindata_list, word_vecs)\n","testembed_list = Word2Vector(testdata_list, word_vecs)\n","\n","# CRF - Train Data (Augmentation Data)\n","x_train = Feature(trainembed_list, ptrain)\n","y_train = Preprocess(traindata_list)\n","\n","# CRF - Test Data (Golden Standard)\n","x_test = Feature(testembed_list, ptest)\n","y_test = Preprocess(testdata_list)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["1 \\ 18 , # of token: 1759\n","2 \\ 18 , # of token: 1827\n","3 \\ 18 , # of token: 1522\n","4 \\ 18 , # of token: 2519\n","5 \\ 18 , # of token: 2618\n","6 \\ 18 , # of token: 1840\n","7 \\ 18 , # of token: 1287\n","8 \\ 18 , # of token: 1000\n","9 \\ 18 , # of token: 1077\n","10 \\ 18 , # of token: 1686\n","11 \\ 18 , # of token: 2024\n","12 \\ 18 , # of token: 3689\n","13 \\ 18 , # of token: 912\n","14 \\ 18 , # of token: 1988\n","15 \\ 18 , # of token: 1080\n","16 \\ 18 , # of token: 1074\n","17 \\ 18 , # of token: 2130\n","1 \\ 10 , # of token: 2829\n","2 \\ 10 , # of token: 1090\n","3 \\ 10 , # of token: 2882\n","4 \\ 10 , # of token: 2075\n","5 \\ 10 , # of token: 1394\n","6 \\ 10 , # of token: 992\n","7 \\ 10 , # of token: 1185\n","8 \\ 10 , # of token: 2731\n","9 \\ 10 , # of token: 1830\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(len(ptrain))\n","print(len(ptrain[0]))\n","print(len(ptrain[0][0]))\n","\n","print(len(trainembed_list))\n","print(len(trainembed_list[0]))\n","print(len(trainembed_list[0][0]))"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["17\n1759\n1\n17\n1759\n512\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# each word with 521 dimension\n","print(x_train[0][1]['dim_1'])\n","print(len(x_train[0][1]))\n","print(y_train[0][0])"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["0.803237\n513\nO\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n\n  B-location      0.000     0.000     0.000        15\n  I-location      0.000     0.000     0.000        41\n  B-med_exam      0.000     0.000     0.000        33\n  I-med_exam      1.000     0.025     0.049        80\n     B-money      0.444     0.333     0.381        12\n     I-money      0.462     0.171     0.250        35\n      B-name      0.000     0.000     0.000         7\n      I-name      0.000     0.000     0.000        10\n      B-time      0.618     0.423     0.503       111\n      I-time      0.828     0.528     0.645       265\n\n   micro avg      0.716     0.327     0.449       609\n   macro avg      0.335     0.148     0.183       609\nweighted avg      0.640     0.327     0.401       609\n\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["f1score"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4006377684321067"]},"metadata":{},"execution_count":39}]},{"metadata":{"trusted":true},"cell_type":"code","source":[],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}