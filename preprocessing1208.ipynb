{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import tensorflow as tf\n",
    "import ckiptagger"
   ]
  },
  {
   "source": [
    "# 讀取訓練與開發資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path= os.getcwd() + '\\\\raw_data\\\\train_2.txt'\n",
    "dev_path = os.getcwd() + '\\\\raw_data\\\\development_2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputFile(train_path, dev_path):\n",
    "    developeset = list()  # store developeset [content,content,...]\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf8') as f:\n",
    "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    \n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    for data in datas:\n",
    "        data=data.split('\\n')\n",
    "        content=data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations=data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]]=annot[4]\n",
    "    \n",
    "    with open(dev_path, 'r', encoding='utf8') as f:\n",
    "    \n",
    "        while(True):\n",
    "            article_id = f.readline()\n",
    "            if 'article_id:' not in article_id:\n",
    "                break\n",
    "            else:\n",
    "                developeset.append(f.readline())\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            \n",
    "\n",
    "    return developeset, trainingset, position, mentions"
   ]
  },
  {
   "source": [
    "# 使用pretrained的模型ckip來標記詞性作為feature\n",
    "[需要使用的模型](http://ckip.iis.sinica.edu.tw/data/ckiptagger/data.zip)\n",
    "ckipath為解壓縮後資料夾的path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import WS, POS, NER\n",
    "ckipath = '.\\\\data\\\\data\\\\data'\n",
    "ws = WS(ckipath)\n",
    "pos = POS(ckipath)\n",
    "# ner = NER(ckipath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def LoadPosTag(trainingset, developmentset):\n",
    "\n",
    "    train_POSTarr = list()\n",
    "    dev_POSTarr = list()\n",
    "\n",
    "    train_len = list()\n",
    "    dev_len = list()\n",
    "\n",
    "    # combin all article\n",
    "    full_article = list()\n",
    "    full_article = trainingset.copy()\n",
    "    for dev_article in developmentset:\n",
    "        full_article.append(dev_article)\n",
    "\n",
    "    word_s = ws(full_article,\n",
    "                sentence_segmentation=True,\n",
    "                segment_delimiter_set={'?', '？', '!', '！', '。', ',',   \n",
    "                                    '，', ';', ':', '、'})\n",
    "    word_p = pos(word_s)\n",
    "    print('CKIP-tag complete')\n",
    "    \n",
    "    for idx_list in range(len(word_s)):\n",
    "        temp_arr = list()\n",
    "        temp_len = list()\n",
    "        for idx_tuple in range(len(word_s[idx_list])):\n",
    "            for character in word_s[idx_list][idx_tuple]:\n",
    "                temp_len.append(len(word_s[idx_list][idx_tuple]))\n",
    "                temp_arr.append(word_p[idx_list][idx_tuple])\n",
    "        # is training set\n",
    "        if idx_list < len(trainingset):\n",
    "            train_POSTarr.append(temp_arr)\n",
    "            train_len.append(temp_len)\n",
    "        else:\n",
    "             dev_POSTarr.append(temp_arr)\n",
    "             dev_len.append(temp_len)\n",
    "        \n",
    "        if (idx_list)%10==0 and idx_list != 0:\n",
    "            print(idx_list, 'completed')\n",
    "\n",
    "    return train_POSTarr, dev_POSTarr, train_len, dev_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WritePOSTagFile(dev_set, train_set, dev_POS, train_POS, dev_len, train_len, dev_file_name='dev_POSTag.txt', train_file_name='train_POSTag.txt'):\n",
    "\n",
    "    alphabet_list = list(string.ascii_uppercase)\n",
    "\n",
    "    # write dev file\n",
    "    if (os.path.isfile(\".\\\\processed_data\\\\\" + dev_file_name)):\n",
    "        os.remove(\".\\\\processed_data\\\\\" + dev_file_name)\n",
    "    with open(\".\\\\processed_data\\\\\" + dev_file_name, \"w\", encoding='utf8') as f:\n",
    "        f.write('article_id,entity_text,POS,LEN\\n')\n",
    "        iter = 0\n",
    "\n",
    "        for idx_dev in range(len(dev_set)):\n",
    "            for idx_text_POSTag in range(len(dev_set[idx_dev])):\n",
    "\n",
    "                # chinese number or digit\n",
    "                if dev_POS[idx_dev][idx_text_POSTag] == 'Neu':\n",
    "                    if dev_set[idx_dev][idx_text_POSTag].isdigit():\n",
    "                        dev_POS[idx_dev][idx_text_POSTag] = 'Neu_digit'\n",
    "                    elif dev_set[idx_dev][idx_text_POSTag] == '.' or dev_set[idx_dev][idx_text_POSTag] == '．':\n",
    "                        dev_POS[idx_dev][idx_text_POSTag] = 'Neu_DOT'\n",
    "                    else:\n",
    "                        dev_POS[idx_dev][idx_text_POSTag] = 'Neu_chinese'\n",
    "\n",
    "                # alphabet or character\n",
    "                if dev_POS[idx_dev][idx_text_POSTag] == 'FW':\n",
    "                    half_width_upper_form = unicodedata.normalize('NFKC', dev_set[idx_dev][idx_text_POSTag]).upper()\n",
    "                    if half_width_upper_form in alphabet_list:\n",
    "                        dev_POS[idx_dev][idx_text_POSTag] = 'FW_' + half_width_upper_form\n",
    "                    elif dev_set[idx_dev][idx_text_POSTag] == '.' or dev_set[idx_dev][idx_text_POSTag] == '．':\n",
    "                        dev_POS[idx_dev][idx_text_POSTag] = 'DOT'\n",
    "                    elif dev_set[idx_dev][idx_text_POSTag] == '齁' or dev_set[idx_dev][idx_text_POSTag] == '～':\n",
    "                        dev_POS[idx_dev][idx_text_POSTag] = 'HUH'\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                f.write(str(iter)+','+dev_set[idx_dev][idx_text_POSTag]+','+dev_POS[idx_dev][idx_text_POSTag]+','+str(dev_len[idx_dev][idx_text_POSTag])+'\\n')\n",
    "            iter = iter + 1\n",
    "        \n",
    "        if iter%10 == 0:\n",
    "            print('Total complete development articles:', iter)\n",
    "\n",
    "    # write train file(identical)\n",
    "    if (os.path.isfile(\".\\\\processed_data\\\\\" + train_file_name)):\n",
    "        os.remove(\".\\\\processed_data\\\\\" + train_file_name)\n",
    "    with open(\".\\\\processed_data\\\\\" + train_file_name, \"w\", encoding='utf8') as f:\n",
    "        f.write('article_id,entity_text,POS,LEN\\n')\n",
    "        iter = 0\n",
    "\n",
    "        for idx_train in range(len(train_set)):\n",
    "            for idx_text_POSTag in range(len(train_set[idx_train])):\n",
    "                # chinese number or digit\n",
    "                if train_POS[idx_train][idx_text_POSTag] == 'Neu':\n",
    "                    if train_set[idx_train][idx_text_POSTag].isdigit():\n",
    "                        train_POS[idx_train][idx_text_POSTag] = 'Neu_digit'\n",
    "                    elif train_set[idx_train][idx_text_POSTag] == '.' or train_set[idx_train][idx_text_POSTag] == '．':\n",
    "                        train_POS[idx_train][idx_text_POSTag] = 'Neu_DOT'\n",
    "                    else:\n",
    "                        train_POS[idx_train][idx_text_POSTag] = 'Neu_chinese'\n",
    "\n",
    "                # alphabet or character\n",
    "                if train_POS[idx_train][idx_text_POSTag] == 'FW':\n",
    "                    half_width_upper_form = unicodedata.normalize('NFKC', train_set[idx_train][idx_text_POSTag]).upper()\n",
    "                    if half_width_upper_form in alphabet_list:\n",
    "                        train_POS[idx_train][idx_text_POSTag] = 'FW_' + half_width_upper_form\n",
    "                    elif train_set[idx_train][idx_text_POSTag] == '.' or train_set[idx_train][idx_text_POSTag] == '．':\n",
    "                        train_POS[idx_train][idx_text_POSTag] = 'DOT'\n",
    "                    elif train_set[idx_train][idx_text_POSTag] == '齁' or train_set[idx_train][idx_text_POSTag] == '～':\n",
    "                        train_POS[idx_train][idx_text_POSTag] = 'HUH'\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                f.write(str(iter)+','+train_set[idx_train][idx_text_POSTag]+','+train_POS[idx_train][idx_text_POSTag]+','+str(train_len[idx_train][idx_text_POSTag])+'\\n')\n",
    "            iter = iter + 1\n",
    "        \n",
    "        if iter%10 == 0:\n",
    "            print('Total complete train articles:', iter)"
   ]
  },
  {
   "source": [
    "# 將資料標記為CRF的label形式(同baseline)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', article_id)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CKIP-tag complete\n",
      "10 completed\n",
      "20 completed\n",
      "30 completed\n",
      "40 completed\n",
      "50 completed\n",
      "60 completed\n",
      "70 completed\n",
      "80 completed\n",
      "90 completed\n",
      "100 completed\n",
      "110 completed\n",
      "120 completed\n",
      "130 completed\n",
      "140 completed\n",
      "150 completed\n",
      "160 completed\n",
      "170 completed\n",
      "180 completed\n",
      "190 completed\n",
      "200 completed\n",
      "210 completed\n",
      "220 completed\n",
      "230 completed\n",
      "240 completed\n",
      "250 completed\n",
      "260 completed\n"
     ]
    }
   ],
   "source": [
    "developset, trainingset, position, mention = loadInputFile(train_path, dev_path)\n",
    "train_POS, dev_POS, train_len, dev_len = LoadPosTag(trainingset, developset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of tagged train article\t 200\n# of tagged text in article0\t 4211\n# of text in article0\t\t 4211\n# of tagged development article\t 70\n# of tagged text in article0\t 3374\n# of text in article0\t\t 3374\n"
     ]
    }
   ],
   "source": [
    "# length checking\n",
    "print('# of tagged train article\\t', len(train_POS))\n",
    "print('# of tagged text in article0\\t', len(train_POS[0]))\n",
    "print('# of text in article0\\t\\t', len(trainingset[0]))\n",
    "print('# of tagged development article\\t', len(dev_POS))\n",
    "print('# of tagged text in article0\\t',len(dev_POS[0]))\n",
    "print('# of text in article0\\t\\t',len(developset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(developset)):\n",
    "    print(len(developset[i]) - len(dev_POS[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total complete development articles: 70\n",
      "Total complete train articles: 200\n"
     ]
    }
   ],
   "source": [
    "WritePOSTagFile(developset, trainingset, dev_POS, train_POS, dev_len, train_len, 'dev2_POSTag.txt', 'train2_POSTag.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n",
      "Total complete articles: 30\n",
      "Total complete articles: 40\n",
      "Total complete articles: 50\n",
      "Total complete articles: 60\n",
      "Total complete articles: 70\n",
      "Total complete articles: 80\n",
      "Total complete articles: 90\n",
      "Total complete articles: 100\n",
      "Total complete articles: 110\n",
      "Total complete articles: 120\n",
      "Total complete articles: 130\n",
      "Total complete articles: 140\n",
      "Total complete articles: 150\n",
      "Total complete articles: 160\n",
      "Total complete articles: 170\n",
      "Total complete articles: 180\n",
      "Total complete articles: 190\n"
     ]
    }
   ],
   "source": [
    "# write CRF label file at data_path\n",
    "data_path='data/train2_sample.data'\n",
    "CRFFormatData(trainingset, position, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "POSTag type of development set\t 84\n",
      "POSTag type of tarining set\t 86\n",
      "Union set type\t 87\n",
      "dev ^ -train:  {'WHITESPACE'}\n",
      "train ^ -dev:  {'DM', 'FW_J', 'FW_Z'}\n"
     ]
    }
   ],
   "source": [
    "# join two set\n",
    "dev_set = set()\n",
    "for article_POS in dev_POS:\n",
    "    for text_POS in article_POS:\n",
    "        dev_set.add(text_POS)\n",
    "print('POSTag type of development set\\t', len(dev_set))\n",
    "\n",
    "train_set = set()\n",
    "for article_POS in train_POS:\n",
    "    for text_POS in article_POS:\n",
    "        train_set.add(text_POS)\n",
    "print('POSTag type of tarining set\\t', len(train_set))\n",
    "\n",
    "union = {*dev_set, *train_set}\n",
    "print('Union set type\\t', len(union))\n",
    "print('dev ^ -train: ', dev_set-train_set)\n",
    "print('train ^ -dev: ', train_set-dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data\\\\trian2_AND_dev2_cat.txt', 'w', encoding='utf-8') as f:\n",
    "    union = list(union)\n",
    "    for u in union:\n",
    "        if u != union[len(union)-1]:\n",
    "            f.write(str(u)+',')\n",
    "        else:\n",
    "            f.write(str(u))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "bc5325771484cd741460cbbcf20a153b1ef45b389ce85e60a186ce6bd8aea41f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}