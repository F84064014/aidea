{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import ckiptagger"
   ]
  },
  {
   "source": [
    "# 讀取訓練與開發資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path= os.getcwd() + '\\\\raw_data\\\\train_2.txt'\n",
    "dev_path = os.getcwd() + '\\\\raw_data\\\\development_2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputFile(train_path, dev_path):\n",
    "    developeset = list()  # store developeset [content,content,...]\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "    \n",
    "    with open(train_path, 'r', encoding='utf8') as f:\n",
    "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    for data in datas:\n",
    "        data=data.split('\\n')\n",
    "        content=data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations=data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]]=annot[4]\n",
    "    \n",
    "    with open(dev_path, 'r', encoding='utf8') as f:\n",
    "    \n",
    "        while(True):\n",
    "            article_id = f.readline()\n",
    "            if 'article_id:' not in article_id:\n",
    "                break\n",
    "            else:\n",
    "                developeset.append(f.readline())\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            f.readline()\n",
    "            \n",
    "\n",
    "    return developeset, trainingset, position, mentions"
   ]
  },
  {
   "source": [
    "# 使用pretrained的模型ckip來標記詞性作為feature\n",
    "[需要使用的模型](http://ckip.iis.sinica.edu.tw/data/ckiptagger/data.zip)\n",
    "ckipath為解壓縮後資料夾的path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import WS, POS, NER\n",
    "ckipath = '.\\\\data\\\\data\\\\data'\n",
    "ws = WS(ckipath)\n",
    "pos = POS(ckipath)\n",
    "# ner = NER(ckipath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def LoadPosTag(trainingset, developmentset):\n",
    "\n",
    "    train_POSTarr = list()\n",
    "    dev_POSTarr = list()\n",
    "\n",
    "    # combin all article\n",
    "    full_article = list()\n",
    "    full_article = trainingset.copy()\n",
    "    for dev_article in developmentset:\n",
    "        full_article.append(dev_article)\n",
    "\n",
    "    word_s = ws(full_article,\n",
    "                sentence_segmentation=True,\n",
    "                segment_delimiter_set={'?', '？', '!', '！', '。', ',',   \n",
    "                                    '，', ';', ':', '、'})\n",
    "    word_p = pos(word_s)\n",
    "    print('CKIP-tag complete')\n",
    "    \n",
    "    for idx_list in range(len(word_s)):\n",
    "        temp_arr = list()\n",
    "        for idx_tuple in range(len(word_s[idx_list])):\n",
    "            for character in word_s[idx_list][idx_tuple]:\n",
    "                temp_arr.append(word_p[idx_list][idx_tuple])\n",
    "        # is training set\n",
    "        if idx_list < len(trainingset):\n",
    "            train_POSTarr.append(temp_arr)\n",
    "        else:\n",
    "             dev_POSTarr.append(temp_arr)\n",
    "        \n",
    "        if (idx_list)%10==0 and idx_list != 0:\n",
    "            print(idx_list, 'completed')\n",
    "\n",
    "    return train_POSTarr, dev_POSTarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WritePOSTagFile(dev_set, train_set, dev_POS, train_POS,dev_file_name='dev_POSTag.txt', train_file_name='train_POSTag.txt'):\n",
    "\n",
    "    # write dev file\n",
    "    if (os.path.isfile(\".\\\\processed_data\\\\\" + dev_file_name)):\n",
    "        os.remove(\".\\\\processed_data\\\\\" + dev_file_name)\n",
    "    with open(\".\\\\processed_data\\\\\" + dev_file_name, \"w\", encoding='utf8') as f:\n",
    "        f.write('article_id,entity_text,POS\\n')\n",
    "        iter = 0\n",
    "\n",
    "        for idx_dev in range(len(dev_set)):\n",
    "            for idx_text_POSTag in range(len(dev_set[idx_dev])):\n",
    "                f.write(str(iter)+','+dev_set[idx_dev][idx_text_POSTag]+','+dev_POS[idx_dev][idx_text_POSTag]+'\\n')\n",
    "            iter = iter + 1\n",
    "        \n",
    "        if iter%10 == 0:\n",
    "            print('Total complete development articles:', iter)\n",
    "\n",
    "    # write train file\n",
    "    if (os.path.isfile(\".\\\\processed_data\\\\\" + train_file_name)):\n",
    "        os.remove(\".\\\\processed_data\\\\\" + train_file_name)\n",
    "    with open(\".\\\\processed_data\\\\\" + train_file_name, \"w\", encoding='utf8') as f:\n",
    "        f.write('article_id,entity_text,POS\\n')\n",
    "        iter = 0\n",
    "\n",
    "        for idx_train in range(len(train_set)):\n",
    "            for idx_text_POSTag in range(len(train_set[idx_train])):\n",
    "                f.write(str(iter)+','+train_set[idx_train][idx_text_POSTag]+','+train_POS[idx_train][idx_text_POSTag]+'\\n')\n",
    "            iter = iter + 1\n",
    "        \n",
    "        if iter%10 == 0:\n",
    "            print('Total complete train articles:', iter)"
   ]
  },
  {
   "source": [
    "# 將資料標記為CRF的label形式(同baseline)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', article_id)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CKIP-tag complete\n10 completed\n20 completed\n30 completed\n40 completed\n50 completed\n60 completed\n70 completed\n80 completed\n90 completed\n100 completed\n110 completed\n120 completed\n130 completed\n140 completed\n150 completed\n160 completed\n170 completed\n180 completed\n190 completed\n200 completed\n210 completed\n220 completed\n230 completed\n240 completed\n250 completed\n260 completed\n"
     ]
    }
   ],
   "source": [
    "developset, trainingset, position, mention = loadInputFile(train_path, dev_path)\n",
    "train_POS, dev_POS = LoadPosTag(trainingset, developset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of tagged train article\t 200\n# of tagged text in article0\t 4211\n# of text in article0\t\t 4211\n# of tagged development article\t 70\n# of tagged text in article0\t 3374\n# of text in article0\t\t 3374\n"
     ]
    }
   ],
   "source": [
    "# length checking\n",
    "print('# of tagged train article\\t', len(train_POS))\n",
    "print('# of tagged text in article0\\t', len(train_POS[0]))\n",
    "print('# of text in article0\\t\\t', len(trainingset[0]))\n",
    "print('# of tagged development article\\t', len(dev_POS))\n",
    "print('# of tagged text in article0\\t',len(dev_POS[0]))\n",
    "print('# of text in article0\\t\\t',len(developset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total complete development articles: 70\n",
      "Total complete train articles: 200\n"
     ]
    }
   ],
   "source": [
    "WritePOSTagFile(developset, trainingset, dev_POS, train_POS, 'dev2_POSTag.txt', 'train2_POSTag.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n",
      "Total complete articles: 30\n",
      "Total complete articles: 40\n",
      "Total complete articles: 50\n",
      "Total complete articles: 60\n",
      "Total complete articles: 70\n",
      "Total complete articles: 80\n",
      "Total complete articles: 90\n",
      "Total complete articles: 100\n",
      "Total complete articles: 110\n",
      "Total complete articles: 120\n",
      "Total complete articles: 130\n",
      "Total complete articles: 140\n",
      "Total complete articles: 150\n",
      "Total complete articles: 160\n",
      "Total complete articles: 170\n",
      "Total complete articles: 180\n",
      "Total complete articles: 190\n"
     ]
    }
   ],
   "source": [
    "# write CRF label file at data_path\n",
    "data_path='data/train2_sample.data'\n",
    "CRFFormatData(trainingset, position, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "POSTag type of development set\t 56\nPOSTag type of tarining set\t 56\n"
     ]
    }
   ],
   "source": [
    "# make sure the feature size is identical\n",
    "dev_set = set()\n",
    "for article_POS in dev_POS:\n",
    "    for text_POS in article_POS:\n",
    "        dev_set.add(text_POS)\n",
    "print('POSTag type of development set\\t', len(dev_set))\n",
    "\n",
    "train_set = set()\n",
    "for article_POS in train_POS:\n",
    "    for text_POS in article_POS:\n",
    "        train_set.add(text_POS)\n",
    "print('POSTag type of tarining set\\t', len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'VJ', 'Nc', 'Cbb', 'V_2', 'Nh', 'COLONCATEGORY', 'PAUSECATEGORY', 'QUESTIONCATEGORY', 'T', 'SHI', 'VG', 'Di', 'PARENTHESISCATEGORY', 'Na', 'DE', 'Nes', 'VD', 'VI', 'Nf', 'VK', 'WHITESPACE', 'Nep', 'VAC', 'VE', 'I', 'Dfb', 'VA', 'Neqb', 'VB', 'VC', 'Ng', 'Neu', 'Dk', 'ETCCATEGORY', 'Neqa', 'VCL', 'COMMACATEGORY', 'P', 'EXCLAMATIONCATEGORY', 'VH', 'Nd', 'Nv', 'Cba', 'Cab', 'Nb', 'Dfa', 'A', 'VL', 'PERIODCATEGORY', 'Ncd', 'VHC', 'D', 'Da', 'FW', 'VF', 'Caa'}\n"
     ]
    }
   ],
   "source": [
    "print(dev_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "bc5325771484cd741460cbbcf20a153b1ef45b389ce85e60a186ce6bd8aea41f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}