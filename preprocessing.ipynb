{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import ckiptagger"
   ]
  },
  {
   "source": [
    "# 讀取訓練用資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= os.getcwd() + '\\\\raw_data\\\\train_2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadInputFile(path):\n",
    "    trainingset = list()  # store trainingset [content,content,...]\n",
    "    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]\n",
    "    mentions = dict()  # store mentions[mention] = Type\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        file_text=f.read().encode('utf-8').decode('utf-8-sig')\n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    for data in datas:\n",
    "        data=data.split('\\n')\n",
    "        content=data[0]\n",
    "        trainingset.append(content)\n",
    "        annotations=data[1:]\n",
    "        for annot in annotations[1:]:\n",
    "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
    "            position.extend(annot)\n",
    "            mentions[annot[3]]=annot[4]\n",
    "    \n",
    "    return trainingset, position, mentions"
   ]
  },
  {
   "source": [
    "# 處理資料的label"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LabelFile(path):\n",
    "    if (os.path.isfile(\"mylabel.txt\")):\n",
    "        os.remove(\"mylabel.txt\")\n",
    "    with open(\"mylabel.txt\", \"w\", encoding='utf8') as fw:\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            fw.write('article_id,start_position,end_position,entity_text,entity_type\\n')\n",
    "            label = []\n",
    "            for i in range(len(trainingset)):\n",
    "                f.readline()  # skip the content\n",
    "                f.readline()  # first line (lable title)\n",
    "                if i != 0:\n",
    "                    f.readline()\n",
    "                while True:\n",
    "                    label_text = f.readline()\n",
    "                    if '--' not in label_text:\n",
    "                        if label_text != '\\n':\n",
    "                            label_text = ','.join(label_text.split()) + '\\n'\n",
    "                            fw.write(label_text)\n",
    "                    else:\n",
    "                        break"
   ]
  },
  {
   "source": [
    "# 使用pretrained的模型ckip來標記詞性作為feature\n",
    "[需要使用的模型](http://ckip.iis.sinica.edu.tw/data/ckiptagger/data.zip)\n",
    "ckipath為解壓縮後資料夾的path"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import WS, POS, NER\n",
    "ckipath = '.\\\\data\\\\data\\\\data'\n",
    "ws = WS(ckipath)\n",
    "pos = POS(ckipath)\n",
    "ner = NER(ckipath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def WritePOSTagFile(article_set, file_name = 'POSTag.txt'):\n",
    "    POST_tag = list()\n",
    "\n",
    "    if (os.path.isfile(\".\\\\processed_data\\\\\" + file_name)):\n",
    "        os.remove(\".\\\\processed_data\\\\\" + file_name)\n",
    "    with open(\".\\\\processed_data\\\\\" + file_name, \"w\", encoding='utf8') as f:\n",
    "        f.write('article_id,entity_text,POS,length\\n')\n",
    "        iter = 0\n",
    "        for article in article_set:    \n",
    "            word_s = ws([article],\n",
    "                        sentence_segmentation=True,\n",
    "                        segment_delimiter_set={'?', '？', '!', '！', '。', ',',   \n",
    "                                            '，', ';', ':', '、'})\n",
    "            word_p = pos(word_s)\n",
    "\n",
    "            for idx_word_s in range(len(word_s[0])):        # each word in article\n",
    "                for character in word_s[0][idx_word_s]:     # each single character in words\n",
    "                    f.write(str(iter)+','+character+','+word_p[0][idx_word_s]+str(len(word_s[0][idx_word_s]))+'\\n')\n",
    "            iter = iter + 1\n",
    "\n",
    "            if iter%10 == 0:\n",
    "                print('Total complete articles:', iter)"
   ]
  },
  {
   "source": [
    "# 將資料標記為CRF的label形式(同baseline)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', iter)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset, position, mentions=loadInputFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total complete articles: 0\n",
      "Total complete articles: 10\n",
      "Total complete articles: 20\n",
      "Total complete articles: 30\n",
      "Total complete articles: 40\n",
      "Total complete articles: 50\n",
      "Total complete articles: 60\n",
      "Total complete articles: 70\n",
      "Total complete articles: 80\n",
      "Total complete articles: 90\n",
      "Total complete articles: 100\n",
      "Total complete articles: 110\n",
      "Total complete articles: 120\n",
      "Total complete articles: 130\n",
      "Total complete articles: 140\n",
      "Total complete articles: 150\n",
      "Total complete articles: 160\n",
      "Total complete articles: 170\n",
      "Total complete articles: 180\n",
      "Total complete articles: 190\n"
     ]
    }
   ],
   "source": [
    "data_path='data/sample.data'\n",
    "CRFFormatData(trainingset, position, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'article_id' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ddcf80340ebf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mWritePOSTagFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train2_POSTag.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-6ce32ca6df2c>\u001b[0m in \u001b[0;36mWritePOSTagFile\u001b[1;34m(article_set, file_name)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Total complete articles:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'article_id' is not defined"
     ]
    }
   ],
   "source": [
    "WritePOSTagFile(trainingset, 'train2_POSTag.txt')"
   ]
  },
  {
   "source": [
    "# trainingset示範"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.醫師：啊回去還好嗎？民眾：欸，還是虛虛的，但。醫師：欸，真的......一百五十塊一點點車馬費。民眾：這麼好喔。\n1.醫師：阿阿嬤她好像說有，前天又有在發燒喔。家屬：對阿都，有時......。醫師：好，ok。家屬：謝謝。醫師：好。\n2.民眾：也有點不舒服，可是就是腰這邊有也一點點痛，我脫起來我想......：那樣這邊就可以了。醫師：那個是回診單。\n3.醫師：謝謝你這樣幫忙他們這樣，那最近還好嗎？民眾：就是因為不......會拿過去，啊你先後面先稍等。醫師：ＯＫ。\n4.醫師：那個，吃藥還Ok嗎？民眾：OK。醫師：沒什麼問題？民眾......。民眾：恩恩。醫師：假如方便的話也可以。\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('{0}.'.format(i) + trainingset[i][:30] + '......' + trainingset[i][(len(trainingset[i])-20):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['0', '69', '71', '前天', 'time', '0']\n['0', '75', '77', '前天', 'time', '0']\n['0', '738', '740', '85', 'med_exam', '0']\n['0', '741', '744', '102', 'med_exam', '0']\n['0', '809', '811', '前年', 'time', '0']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(position[ (i*5):(i*5+6) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "前天 | time\n前天 | time\n85 | med_exam\n102 | med_exam\n前年 | time\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(position[(i*5+3)],'|', mentions[position[(i*5+3)]])"
   ]
  },
  {
   "source": [
    "# ckip示範"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['醫師', '：', '啊', '回去', '還好', '嗎', '？', '民眾', '：', '欸']\n"
     ]
    }
   ],
   "source": [
    "word_s = ws([trainingset[0]],\n",
    "            sentence_segmentation=True,\n",
    "            segment_delimiter_set={'?', '？', '!', '！', '。', ',',   \n",
    "                                   '，', ';', ':', '、'})\n",
    "print(word_s[0][:10])"
   ]
  },
  {
   "source": [
    "## [中研院平衡與料庫詞類標記集](http://ckipsvr.iis.sinica.edu.tw/papers/category_list.pdf)\n",
    "\n",
    "Na: 普通名詞, Nb: 專有名詞, ..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "醫師:\tNa\n：:\tCOLONCATEGORY\n啊:\tI\n回去:\tVA\n還好:\tVH\n嗎:\tT\n？:\tQUESTIONCATEGORY\n民眾:\tNa\n：:\tCOLONCATEGORY\n欸:\tI\n"
     ]
    }
   ],
   "source": [
    "word_p = pos(word_s)\n",
    "\n",
    "for i in range(10):\n",
    "    print(word_s[0][i]+':\\t'+word_p[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "bc5325771484cd741460cbbcf20a153b1ef45b389ce85e60a186ce6bd8aea41f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}