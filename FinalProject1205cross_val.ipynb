{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sklearn_crfsuite in c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.3.6)\n","Requirement already satisfied: tqdm>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from sklearn_crfsuite) (4.45.0)\n","Requirement already satisfied: six in c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from sklearn_crfsuite) (1.14.0)\n","Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from sklearn_crfsuite) (0.9.7)\n","Requirement already satisfied: tabulate in c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from sklearn_crfsuite) (0.8.7)\n","WARNING: You are using pip version 20.2.4; however, version 20.3.1 is available.\n","You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python36\\python.exe -m pip install --upgrade pip' command.\n"]}],"source":["!pip3 install sklearn_crfsuite"]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import os\n","import sys\n","import unicodedata\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","from sklearn_crfsuite.metrics import flat_classification_report"],"execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def CRF(x_train, y_train, x_test, y_test):\n","    crf = sklearn_crfsuite.CRF(\n","        algorithm='lbfgs',\n","        c1=0.1,\n","        c2=0.1,\n","        max_iterations=100,\n","        all_possible_transitions=True\n","    )\n","    crf.fit(x_train, y_train)\n","#     print(crf) #\n","    y_pred = crf.predict(x_test)\n","    y_pred_mar = crf.predict_marginals(x_test)\n","\n","#     print(y_pred_mar) #\n","\n","    labels = list(crf.classes_)\n","    labels.remove('O')\n","    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n","    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n","    print(flat_classification_report(y_test, y_pred, labels=sorted_labels, digits=3))\n","    \n","#     eli5.show_weights(crf, top=10, feature_re='^word\\.is',\n","#                       horizontal_layout=False, show=['targets'])\n","    \n","    return y_pred, y_pred_mar, f1score, crf"],"execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# load pretrained word vectors\n","# get a dict of tokens (key) and their pretrained word vectors (value)\n","# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n","dim = 0\n","word_vecs= {}\n","# open pretrained word vector file\n","with open('.\\\\raw_data\\\\cna.cbow.cwe_p.tar_g.512d.0.txt', encoding='utf-8') as f:\n","    for line in f:\n","        tokens = line.strip().split()\n","\n","        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n","        if len(tokens) == 2:\n","            dim = int(tokens[1])\n","            continue\n","    \n","        word = tokens[0]\n","        vec = np.array([ float(t) for t in tokens[1:] ])\n","        word_vecs[word] = vec"],"execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["vocabulary_size:  158566  word_vector_dim:  (512,)\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# load `train.data` and separate into a list of labeled data of each text\n","# return:\n","#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n","#   traindata_list: a list of lists, storing training data_list splitted from data_list\n","#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n","from sklearn.model_selection import train_test_split\n","def Dataset(data_path):\n","    with open(data_path, 'r', encoding='utf-8') as f:\n","        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n","    data_list, data_list_tmp = list(), list()\n","    article_id_list=list()\n","    idx=0\n","    for row in data:\n","        data_tuple = tuple()\n","        if row == '\\n':\n","            article_id_list.append(idx)\n","            idx+=1\n","            data_list.append(data_list_tmp)\n","            data_list_tmp = []\n","        else:\n","            row = row.strip('\\n').split(' ')\n","            data_tuple = (row[0], row[1])\n","            data_list_tmp.append(data_tuple)\n","    if len(data_list_tmp) != 0:\n","        data_list.append(data_list_tmp)\n","    \n","    # here we random split data into training dataset and testing dataset\n","    # but you should take `development data` or `test data` as testing data\n","    # At that time, you could just delete this line, \n","    # and generate data_list of `train data` and data_list of `development/test data` by this function\n","    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n","                                                                                                    article_id_list,\n","                                                                                                    test_size=0.33,\n","                                                                                                    random_state=56)\n","    \n","    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "],"execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# open the pos tag file\n","df_POSTag = pd.read_csv('.\\\\processed_data\\\\train2_POSTag.txt')\n","df_POSTag.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   article_id entity_text             POS  length\n","0           0           醫             Na2     NaN\n","1           0           師             Na2     NaN\n","2           0           ：  COLONCATEGORY1     NaN\n","3           0           啊              I1     NaN\n","4           0           回             VA2     NaN"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article_id</th>\n      <th>entity_text</th>\n      <th>POS</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>醫</td>\n      <td>Na2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>師</td>\n      <td>Na2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>：</td>\n      <td>COLONCATEGORY1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>啊</td>\n      <td>I1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>回</td>\n      <td>VA2</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":7}]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(len(df_POSTag['entity_text'].unique()))\n","print(len(df_POSTag))\n","print(len(df_POSTag['POS'].unique()))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2140\n415530\n181\n"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["df_POSTag.drop_duplicates(inplace=True)\n","df_POSTag.head()\n","text_pool = set(df_POSTag['entity_text'])"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# look up the POSTag txt\n","# encode the word\n","def POSTagEncode(data_list, article_id_list):\n","    \n","    \n","    POSTag_list = list()\n","\n","    df = pd.read_csv('.\\\\processed_data\\\\train2_POSTag.txt')\n","    df = df[df['entity_text'] != ' ']\n","    POSTag_label = list(df['POS'].unique())\n","\n","    for idx_list in range(len(data_list)):\n","        df_temp = df[df['article_id']==article_id_list[idx_list]]\n","        text_temp = list(df_temp['entity_text'])\n","        POS_temp  = list(df_temp['POS'])\n","        print(article_id_list[idx_list],'\\'',str(len(data_list[idx_list])-len(POS_temp)))\n","        POSTag_list_temp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            word_POSTag_temp = list()\n","            for POSTag_label_code in POSTag_label:\n","                if POS_temp[idx_tuple] == POSTag_label_code:\n","                    word_POSTag_temp.append(1)\n","                else:\n","                    word_POSTag_temp.append(0)\n","                    \n","            POSTag_list_temp.append(word_POSTag_temp)\n","        POSTag_list.append(POSTag_list_temp)\n","\n","    return POSTag_list"],"execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# look up word vectors\n","# turn each word into its pretrained word vector\n","# return a list of word vectors corresponding to each token in train.data\n","def Word2Vector(data_list, embedding_dict):\n","    embedding_list = list()\n","\n","    # No Match Word (unknown word) Vector in Embedding\n","    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n","\n","    for idx_list in range(len(data_list)):\n","        embedding_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            key = data_list[idx_list][idx_tuple][0] # token\n","\n","            if key in embedding_dict:\n","                value = embedding_dict[key]\n","            else:\n","                value = unk_vector\n","            embedding_list_tmp.append(value)\n","        embedding_list.append(embedding_list_tmp)\n","    \n","    return embedding_list"],"execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# input features: pretrained word vectors of each token\n","# return a list of feature dicts, each feature dict corresponding to each token\n","def Feature(embed_list, p):\n","    \n","    df = pd.read_csv('.\\\\processed_data\\\\train2_POSTag.txt')\n","    POS_unique_list = list(df['POS'].unique())\n","    # alphabet_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","    # for alpha in alphabet_list:\n","    #     POS_unique_list.append(alpha)\n","    POS_unique_list.append('Start') # 1 if no last word\n","    \n","    feature_list = list()\n","    \n","    # feature of w2d (original)\n","    for idx_list in range(len(embed_list)):\n","        feature_list_tmp = list()\n","        for idx_tuple in range(len(embed_list[idx_list])):\n","            \n","            feature_dict = dict()\n","            \n","            # feature of word's POSTag(55+55+1)\n","            feature_dict['Start'] = 0\n","            for idx_POS in range(len(POS_unique_list)-1): # exclude Start\n","                feature_dict[POS_unique_list[idx_POS]] = p[idx_list][idx_tuple][idx_POS]\n","                if idx_tuple != 0:\n","                    feature_dict['last_' + POS_unique_list[idx_POS]] = p[idx_list][idx_tuple-1][idx_POS]\n","                else:\n","                    feature_dict['Start'] = 1\n","\n","            #-----------------\n","            # feature of word's vector(512)\n","            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n","                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n","\n","            feature_list_tmp.append(feature_dict)\n","        feature_list.append(feature_list_tmp)\n","        print(idx_list+1, '\\\\', len(embed_list)+1, ', # of token:', len(embed_list[idx_list]))\n","        \n","    return feature_list"],"execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# get the labels of each tokens in train.data\n","# return a list of lists of labels\n","def Preprocess(data_list):\n","    label_list = list()\n","    for idx_list in range(len(data_list)):\n","        label_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n","        label_list.append(label_list_tmp)\n","    return label_list"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset('.\\\\data\\\\train2_sample.data')"],"execution_count":14,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0 ' 0\n","1 ' 0\n","2 ' 0\n","3 ' 0\n","4 ' 0\n","5 ' 0\n","6 ' 0\n","7 ' 0\n","8 ' 0\n","9 ' 0\n","10 ' 0\n","11 ' 0\n","12 ' 0\n","13 ' 0\n","14 ' 0\n","15 ' 0\n","16 ' 0\n","17 ' 0\n","18 ' 0\n","19 ' 0\n","20 ' 0\n","21 ' 0\n","22 ' 0\n","23 ' 0\n","24 ' 0\n","25 ' 0\n","26 ' 0\n","27 ' 0\n","28 ' 0\n","29 ' 0\n","30 ' 0\n","31 ' 0\n","32 ' 0\n","33 ' 0\n","34 ' 0\n","35 ' 0\n","36 ' 0\n","37 ' 0\n","38 ' 0\n","39 ' 0\n","40 ' 0\n","41 ' 0\n","42 ' 0\n","43 ' 0\n","44 ' 0\n","45 ' 0\n","46 ' 0\n","47 ' 0\n","48 ' 0\n","49 ' 0\n","50 ' 0\n","51 ' 0\n","52 ' 0\n","53 ' 0\n","54 ' 0\n","55 ' 0\n","56 ' 0\n","57 ' 0\n","58 ' 0\n","59 ' 0\n","60 ' 0\n","61 ' 0\n","62 ' 0\n","63 ' 0\n","64 ' 0\n","65 ' 0\n","66 ' 0\n","67 ' 0\n","68 ' 0\n","69 ' 0\n","70 ' 0\n","71 ' 0\n","72 ' 0\n","73 ' 0\n","74 ' 0\n","75 ' 0\n","76 ' 0\n","77 ' 0\n","78 ' 0\n","79 ' 0\n","80 ' 0\n","81 ' 0\n","82 ' 0\n","83 ' 0\n","84 ' 0\n","85 ' 0\n","86 ' 0\n","87 ' 0\n","88 ' 0\n","89 ' 0\n","90 ' 0\n","91 ' 0\n","92 ' 0\n","93 ' 0\n","94 ' 0\n","95 ' 0\n","96 ' 0\n","97 ' 0\n","98 ' 0\n","99 ' 0\n","100 ' 0\n","101 ' 0\n","102 ' 0\n","103 ' 0\n","104 ' 0\n","105 ' 0\n","106 ' 0\n","107 ' 0\n","108 ' 0\n","109 ' 0\n","110 ' 0\n","111 ' 0\n","112 ' 0\n","113 ' 0\n","114 ' 0\n","115 ' 0\n","116 ' 0\n","117 ' 0\n","118 ' 0\n","119 ' 0\n","120 ' 0\n","121 ' 0\n","122 ' 0\n","123 ' 0\n","124 ' 0\n","125 ' 0\n","126 ' 0\n","127 ' 0\n","128 ' 0\n","129 ' 0\n","130 ' 0\n","131 ' 0\n","132 ' 0\n","133 ' 0\n","134 ' 0\n","135 ' 0\n","136 ' 0\n","137 ' 0\n","138 ' 0\n","139 ' 0\n","140 ' 0\n","141 ' 0\n","142 ' 0\n","143 ' 0\n","144 ' 0\n","145 ' 0\n","146 ' 0\n","147 ' 0\n","148 ' 0\n","149 ' 0\n","150 ' 0\n","151 ' 0\n","152 ' 0\n","153 ' 0\n","154 ' 0\n","155 ' 0\n","156 ' 0\n","157 ' 0\n","158 ' 0\n","159 ' 0\n","160 ' 0\n","161 ' 0\n","162 ' 0\n","163 ' 0\n","164 ' 0\n","165 ' 0\n","166 ' 0\n","167 ' 0\n","168 ' 0\n","169 ' 0\n","170 ' 0\n","171 ' 0\n","172 ' 0\n","173 ' 0\n","174 ' 0\n","175 ' 0\n","176 ' 0\n","177 ' 0\n","178 ' 0\n","179 ' 0\n","180 ' 0\n","181 ' 0\n","182 ' 0\n","183 ' 0\n","184 ' 0\n","185 ' 0\n","186 ' 0\n","187 ' 0\n","188 ' 0\n","189 ' 0\n","190 ' 0\n","191 ' 0\n","192 ' 0\n","193 ' 0\n","194 ' 0\n","195 ' 0\n","196 ' 0\n","197 ' 0\n","198 ' 0\n","199 ' 0\n","1 \\ 201 , # of token: 4211\n","2 \\ 201 , # of token: 2652\n","3 \\ 201 , # of token: 1689\n","4 \\ 201 , # of token: 1229\n","5 \\ 201 , # of token: 767\n","6 \\ 201 , # of token: 567\n","7 \\ 201 , # of token: 3101\n","8 \\ 201 , # of token: 1790\n","9 \\ 201 , # of token: 1134\n","10 \\ 201 , # of token: 2213\n","11 \\ 201 , # of token: 710\n","12 \\ 201 , # of token: 3503\n","13 \\ 201 , # of token: 1933\n","14 \\ 201 , # of token: 1229\n","15 \\ 201 , # of token: 1340\n","16 \\ 201 , # of token: 1645\n","17 \\ 201 , # of token: 3635\n","18 \\ 201 , # of token: 1204\n","19 \\ 201 , # of token: 2051\n","20 \\ 201 , # of token: 2603\n","21 \\ 201 , # of token: 1282\n","22 \\ 201 , # of token: 1809\n","23 \\ 201 , # of token: 3463\n","24 \\ 201 , # of token: 1633\n","25 \\ 201 , # of token: 1702\n","26 \\ 201 , # of token: 1597\n","27 \\ 201 , # of token: 3369\n","28 \\ 201 , # of token: 4294\n","29 \\ 201 , # of token: 1588\n","30 \\ 201 , # of token: 1495\n","31 \\ 201 , # of token: 3755\n","32 \\ 201 , # of token: 1142\n","33 \\ 201 , # of token: 2970\n","34 \\ 201 , # of token: 1760\n","35 \\ 201 , # of token: 956\n","36 \\ 201 , # of token: 2459\n","37 \\ 201 , # of token: 5276\n","38 \\ 201 , # of token: 882\n","39 \\ 201 , # of token: 2030\n","40 \\ 201 , # of token: 1871\n","41 \\ 201 , # of token: 1596\n","42 \\ 201 , # of token: 1590\n","43 \\ 201 , # of token: 1163\n","44 \\ 201 , # of token: 2976\n","45 \\ 201 , # of token: 1738\n","46 \\ 201 , # of token: 2453\n","47 \\ 201 , # of token: 2099\n","48 \\ 201 , # of token: 1607\n","49 \\ 201 , # of token: 3458\n","50 \\ 201 , # of token: 1215\n","51 \\ 201 , # of token: 2609\n","52 \\ 201 , # of token: 1718\n","53 \\ 201 , # of token: 581\n","54 \\ 201 , # of token: 1047\n","55 \\ 201 , # of token: 1694\n","56 \\ 201 , # of token: 1227\n","57 \\ 201 , # of token: 2519\n","58 \\ 201 , # of token: 2168\n","59 \\ 201 , # of token: 2360\n","60 \\ 201 , # of token: 1182\n","61 \\ 201 , # of token: 1090\n","62 \\ 201 , # of token: 1384\n","63 \\ 201 , # of token: 1375\n","64 \\ 201 , # of token: 1000\n","65 \\ 201 , # of token: 1659\n","66 \\ 201 , # of token: 1198\n","67 \\ 201 , # of token: 661\n","68 \\ 201 , # of token: 1083\n","69 \\ 201 , # of token: 2404\n","70 \\ 201 , # of token: 1686\n","71 \\ 201 , # of token: 1074\n","72 \\ 201 , # of token: 3081\n","73 \\ 201 , # of token: 3689\n","74 \\ 201 , # of token: 1287\n","75 \\ 201 , # of token: 984\n","76 \\ 201 , # of token: 633\n","77 \\ 201 , # of token: 1331\n","78 \\ 201 , # of token: 2348\n","79 \\ 201 , # of token: 1268\n","80 \\ 201 , # of token: 4079\n","81 \\ 201 , # of token: 2430\n","82 \\ 201 , # of token: 1375\n","83 \\ 201 , # of token: 1816\n","84 \\ 201 , # of token: 2075\n","85 \\ 201 , # of token: 2274\n","86 \\ 201 , # of token: 2747\n","87 \\ 201 , # of token: 954\n","88 \\ 201 , # of token: 1157\n","89 \\ 201 , # of token: 1077\n","90 \\ 201 , # of token: 1777\n","91 \\ 201 , # of token: 1764\n","92 \\ 201 , # of token: 1094\n","93 \\ 201 , # of token: 1847\n","94 \\ 201 , # of token: 2075\n","95 \\ 201 , # of token: 4830\n","96 \\ 201 , # of token: 1752\n","97 \\ 201 , # of token: 3233\n","98 \\ 201 , # of token: 2184\n","99 \\ 201 , # of token: 1481\n","100 \\ 201 , # of token: 3335\n","101 \\ 201 , # of token: 2024\n","102 \\ 201 , # of token: 1512\n","103 \\ 201 , # of token: 3159\n","104 \\ 201 , # of token: 3286\n","105 \\ 201 , # of token: 3490\n","106 \\ 201 , # of token: 2678\n","107 \\ 201 , # of token: 1520\n","108 \\ 201 , # of token: 2278\n","109 \\ 201 , # of token: 2824\n","110 \\ 201 , # of token: 1791\n","111 \\ 201 , # of token: 1685\n","112 \\ 201 , # of token: 1399\n","113 \\ 201 , # of token: 7309\n","114 \\ 201 , # of token: 1642\n","115 \\ 201 , # of token: 4065\n","116 \\ 201 , # of token: 5638\n","117 \\ 201 , # of token: 2861\n","118 \\ 201 , # of token: 2615\n","119 \\ 201 , # of token: 3929\n","120 \\ 201 , # of token: 1357\n","121 \\ 201 , # of token: 1345\n","122 \\ 201 , # of token: 2214\n","123 \\ 201 , # of token: 1169\n","124 \\ 201 , # of token: 1039\n","125 \\ 201 , # of token: 4435\n","126 \\ 201 , # of token: 699\n","127 \\ 201 , # of token: 1745\n","128 \\ 201 , # of token: 1259\n","129 \\ 201 , # of token: 3300\n","130 \\ 201 , # of token: 1866\n","131 \\ 201 , # of token: 1352\n","132 \\ 201 , # of token: 977\n","133 \\ 201 , # of token: 2156\n","134 \\ 201 , # of token: 637\n","135 \\ 201 , # of token: 823\n","136 \\ 201 , # of token: 2228\n","137 \\ 201 , # of token: 2191\n","138 \\ 201 , # of token: 1745\n","139 \\ 201 , # of token: 936\n","140 \\ 201 , # of token: 1184\n","141 \\ 201 , # of token: 380\n","142 \\ 201 , # of token: 1678\n","143 \\ 201 , # of token: 1790\n","144 \\ 201 , # of token: 2665\n","145 \\ 201 , # of token: 2509\n","146 \\ 201 , # of token: 1380\n","147 \\ 201 , # of token: 979\n","148 \\ 201 , # of token: 2925\n","149 \\ 201 , # of token: 1466\n","150 \\ 201 , # of token: 3395\n","151 \\ 201 , # of token: 575\n","152 \\ 201 , # of token: 695\n","153 \\ 201 , # of token: 1233\n","154 \\ 201 , # of token: 1901\n","155 \\ 201 , # of token: 7654\n","156 \\ 201 , # of token: 2549\n","157 \\ 201 , # of token: 1728\n","158 \\ 201 , # of token: 1115\n","159 \\ 201 , # of token: 1172\n","160 \\ 201 , # of token: 2882\n","161 \\ 201 , # of token: 739\n","162 \\ 201 , # of token: 3229\n","163 \\ 201 , # of token: 3248\n","164 \\ 201 , # of token: 2011\n","165 \\ 201 , # of token: 4635\n","166 \\ 201 , # of token: 1311\n","167 \\ 201 , # of token: 1595\n","168 \\ 201 , # of token: 1448\n","169 \\ 201 , # of token: 4396\n","170 \\ 201 , # of token: 1326\n","171 \\ 201 , # of token: 2345\n","172 \\ 201 , # of token: 2675\n","173 \\ 201 , # of token: 3731\n","174 \\ 201 , # of token: 2613\n","175 \\ 201 , # of token: 2076\n","176 \\ 201 , # of token: 2402\n","177 \\ 201 , # of token: 2181\n","178 \\ 201 , # of token: 2393\n","179 \\ 201 , # of token: 1550\n","180 \\ 201 , # of token: 3758\n","181 \\ 201 , # of token: 2101\n","182 \\ 201 , # of token: 2690\n","183 \\ 201 , # of token: 3284\n","184 \\ 201 , # of token: 2086\n","185 \\ 201 , # of token: 846\n","186 \\ 201 , # of token: 2882\n","187 \\ 201 , # of token: 2731\n","188 \\ 201 , # of token: 1827\n","189 \\ 201 , # of token: 2618\n","190 \\ 201 , # of token: 1840\n","191 \\ 201 , # of token: 1759\n","192 \\ 201 , # of token: 2130\n","193 \\ 201 , # of token: 912\n","194 \\ 201 , # of token: 2829\n","195 \\ 201 , # of token: 992\n","196 \\ 201 , # of token: 1988\n","197 \\ 201 , # of token: 1394\n","198 \\ 201 , # of token: 1522\n","199 \\ 201 , # of token: 1185\n","200 \\ 201 , # of token: 1080\n","                  precision    recall  f1-score   support\n","\n","            B-ID      0.500     0.333     0.400         3\n","            I-ID      0.455     0.500     0.476        10\n","B-clinical_event      0.000     0.000     0.000         2\n","I-clinical_event      0.000     0.000     0.000         6\n","       B-contact      0.333     0.143     0.200        21\n","       I-contact      0.404     0.277     0.329        83\n","     B-education      0.000     0.000     0.000         4\n","     I-education      0.000     0.000     0.000         7\n","        B-family      0.143     0.167     0.154        12\n","        I-family      0.143     0.167     0.154        12\n","      B-location      0.737     0.538     0.622        78\n","      I-location      0.667     0.418     0.514       110\n","      B-med_exam      0.475     0.564     0.515       133\n","      I-med_exam      0.481     0.701     0.571       274\n","         B-money      0.857     0.247     0.383        73\n","         I-money      0.766     0.268     0.397       183\n","          B-name      0.661     0.577     0.617        71\n","          I-name      0.670     0.607     0.637       117\n","        B-others      0.000     0.000     0.000         0\n","        I-others      0.000     0.000     0.000         0\n","    B-profession      0.000     0.000     0.000        17\n","    I-profession      0.000     0.000     0.000        42\n","          B-time      0.788     0.637     0.705       807\n","          I-time      0.814     0.710     0.758      1718\n","\n","       micro avg      0.721     0.609     0.660      3783\n","       macro avg      0.371     0.286     0.310      3783\n","    weighted avg      0.724     0.609     0.651      3783\n","\n","                  precision    recall  f1-score   support\n","\n","            B-ID      0.000     0.000     0.000         2\n","            I-ID      0.000     0.000     0.000         5\n","B-clinical_event      0.000     0.000     0.000         3\n","I-clinical_event      0.000     0.000     0.000         9\n","       B-contact      1.000     0.111     0.200         9\n","       I-contact      1.000     0.333     0.500        27\n","     B-education      0.000     0.000     0.000         2\n","     I-education      0.000     0.000     0.000         5\n","        B-family      0.120     0.231     0.158        13\n","        I-family      0.133     0.250     0.174        16\n","      B-location      0.583     0.632     0.606        95\n","      I-location      0.518     0.529     0.523       138\n","      B-med_exam      0.598     0.693     0.642       150\n","      I-med_exam      0.614     0.761     0.679       284\n","         B-money      0.654     0.333     0.442        51\n","         I-money      0.625     0.346     0.446       130\n","          B-name      0.759     0.629     0.688       105\n","          I-name      0.796     0.597     0.682       196\n","        B-others      0.000     0.000     0.000         0\n","        I-others      0.000     0.000     0.000         0\n","    B-profession      0.000     0.000     0.000        21\n","    I-profession      0.000     0.000     0.000        51\n","          B-time      0.667     0.719     0.692       771\n","          I-time      0.692     0.820     0.751      1615\n","\n","       micro avg      0.662     0.701     0.681      3698\n","       macro avg      0.365     0.291     0.299      3698\n","    weighted avg      0.652     0.701     0.668      3698\n","\n","                  precision    recall  f1-score   support\n","\n","            B-ID      0.000     0.000     0.000        11\n","            I-ID      0.000     0.000     0.000        40\n","B-clinical_event      0.000     0.000     0.000         2\n","I-clinical_event      0.000     0.000     0.000         6\n","       B-contact      1.000     0.154     0.267        13\n","       I-contact      1.000     0.286     0.444        49\n","     B-education      0.000     0.000     0.000         1\n","     I-education      0.000     0.000     0.000         1\n","        B-family      0.182     0.154     0.167        13\n","        I-family      0.154     0.143     0.148        14\n","      B-location      0.853     0.483     0.617        60\n","      I-location      0.814     0.315     0.455       111\n","      B-med_exam      0.545     0.622     0.581       135\n","      I-med_exam      0.595     0.681     0.635       295\n","         B-money      0.600     0.414     0.490        29\n","         I-money      0.612     0.423     0.500        71\n","          B-name      0.582     0.667     0.621        48\n","          I-name      0.582     0.750     0.655        76\n","  B-organization      0.000     0.000     0.000         0\n","  I-organization      0.000     0.000     0.000         0\n","        B-others      0.000     0.000     0.000         0\n","        I-others      0.000     0.000     0.000         0\n","    B-profession      0.000     0.000     0.000         3\n","    I-profession      0.000     0.000     0.000         4\n","          B-time      0.717     0.686     0.701       627\n","          I-time      0.752     0.782     0.767      1418\n","\n","       micro avg      0.700     0.674     0.686      3027\n","       macro avg      0.346     0.252     0.271      3027\n","    weighted avg      0.696     0.674     0.674      3027\n","\n","                  precision    recall  f1-score   support\n","\n","            B-ID      0.000     0.000     0.000         6\n","            I-ID      0.000     0.000     0.000        15\n","B-clinical_event      0.000     0.000     0.000         1\n","I-clinical_event      0.000     0.000     0.000         3\n","       B-contact      0.444     0.200     0.276        20\n","       I-contact      0.490     0.273     0.350        88\n","     B-education      0.000     0.000     0.000         0\n","     I-education      0.000     0.000     0.000         0\n","        B-family      0.200     0.231     0.214        13\n","        I-family      0.267     0.235     0.250        17\n","      B-location      0.857     0.720     0.783        75\n","      I-location      0.865     0.626     0.726       123\n","      B-med_exam      0.559     0.603     0.580       141\n","      I-med_exam      0.568     0.572     0.570       285\n","         B-money      0.629     0.349     0.449        63\n","         I-money      0.581     0.424     0.490       144\n","          B-name      0.731     0.655     0.691        87\n","          I-name      0.644     0.678     0.660       152\n","  B-organization      0.000     0.000     0.000         0\n","  I-organization      0.000     0.000     0.000         0\n","        B-others      0.000     0.000     0.000         0\n","        I-others      0.000     0.000     0.000         0\n","    B-profession      0.000     0.000     0.000         9\n","    I-profession      0.000     0.000     0.000        15\n","          B-time      0.723     0.656     0.688       733\n","          I-time      0.753     0.729     0.741      1574\n","\n","       micro avg      0.703     0.641     0.671      3564\n","       macro avg      0.320     0.267     0.287      3564\n","    weighted avg      0.693     0.641     0.664      3564\n","\n","                  precision    recall  f1-score   support\n","\n","            B-ID      0.000     0.000     0.000         1\n","            I-ID      0.000     0.000     0.000         3\n","B-clinical_event      0.000     0.000     0.000         1\n","I-clinical_event      0.000     0.000     0.000         3\n","       B-contact      0.250     0.143     0.182        14\n","       I-contact      0.188     0.107     0.136        56\n","     B-education      0.000     0.000     0.000         0\n","     I-education      0.000     0.000     0.000         0\n","        B-family      0.250     0.059     0.095        17\n","        I-family      0.500     0.105     0.174        19\n","      B-location      0.768     0.776     0.772        98\n","      I-location      0.770     0.759     0.764       141\n","      B-med_exam      0.500     0.791     0.613       115\n","      I-med_exam      0.523     0.846     0.647       228\n","         B-money      0.792     0.358     0.494        53\n","         I-money      0.789     0.341     0.476       132\n","          B-name      0.725     0.704     0.714        71\n","          I-name      0.714     0.746     0.730       134\n","    B-profession      0.000     0.000     0.000         6\n","    I-profession      0.000     0.000     0.000        20\n","          B-time      0.763     0.694     0.727       710\n","          I-time      0.793     0.774     0.783      1545\n","\n","       micro avg      0.723     0.707     0.715      3367\n","       macro avg      0.378     0.327     0.332      3367\n","    weighted avg      0.727     0.707     0.707      3367\n","\n"]}],"source":["from sklearn.model_selection import ShuffleSplit\n","rs = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n","crfs = []\n","\n","a = list(range(200))\n","p = POSTagEncode(data_list, a)\n","embed_list = Word2Vector(data_list, word_vecs)\n","X = Feature(embed_list, p)\n","y = Preprocess(data_list)\n","\n","for train_index, test_index in rs.split(data_list):\n","    \n","    x_train = list()\n","    y_train = list()\n","    for idx in train_index:\n","        x_train.append(X[idx])\n","        y_train.append(y[idx])\n","    x_test = list()\n","    y_test = list()\n","    for idx in test_index:\n","        x_test.append(X[idx])\n","        y_test.append(y[idx])\n","\n","    y_pred, y_pred_mar, f1score, crf = CRF(x_train, y_train, x_test, y_test)\n","    crfs.append(crf)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import joblib\n","\n","# save the trained model\n","for idx in range(len(crfs)):\n","    filename = '.\\\\saved_model\\\\crf1208_' + str(idx) + '.sav'\n","    joblib.dump(crfs[idx], filename)"]},{"metadata":{"trusted":true},"cell_type":"code","source":["ptrain = POSTagEncode(traindata_list, traindata_article_id_list)\n","ptest  = POSTagEncode(testdata_list, testdata_article_id_list)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Load Word Embedding\n","trainembed_list = Word2Vector(traindata_list, word_vecs)\n","testembed_list = Word2Vector(testdata_list, word_vecs)\n","\n","# CRF - Train Data (Augmentation Data)\n","x_train = Feature(trainembed_list, ptrain)\n","y_train = Preprocess(traindata_list)\n","\n","# CRF - Test Data (Golden Standard)\n","x_test = Feature(testembed_list, ptest)\n","y_test = Preprocess(testdata_list)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(len(ptrain))\n","print(len(ptrain[0]))\n","print(len(ptrain[0][0]))\n","\n","print(len(trainembed_list))\n","print(len(trainembed_list[0]))\n","print(len(trainembed_list[0][0]))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# each word with 521 dimension\n","print(x_train[0][1]['dim_1'])\n","print(len(x_train[0][1]))\n","print(y_train[0][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# release resources\n","if 'trainembed_list' in globals():\n","    del trainembed_list\n","if 'testembed_list' in globals():\n","    del testembed_list\n","if 'ptrain' in globals():\n","    del ptrain\n","if 'ptest' in globals():\n","    del ptest"]},{"metadata":{"trusted":true},"cell_type":"code","source":["y_pred, y_pred_mar, f1score, crf = CRF(x_train, y_train, x_test, y_test)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["f1score"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if 'x_train' in globals():\n","    del x_train\n","if 'y_train' in globals():\n","    del y_train\n","if 'x_test' in globals():\n","    del x_test\n","if 'y_test' in globals():\n","    del y_test"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.6.7 64-bit","metadata":{"interpreter":{"hash":"bc5325771484cd741460cbbcf20a153b1ef45b389ce85e60a186ce6bd8aea41f"}}},"language_info":{"name":"python","version":"3.6.7-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}