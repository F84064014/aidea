{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":["import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","from sklearn_crfsuite.metrics import flat_classification_report"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def CRF(x_train, y_train):\n","    crf = sklearn_crfsuite.CRF(\n","        algorithm='lbfgs',\n","        c1=0.1,\n","        c2=0.1,\n","        max_iterations=150,\n","        all_possible_transitions=True\n","    )\n","    crf.fit(x_train, y_train)\n","    labels = list(crf.classes_)\n","    \n","    return labels, crf"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# load pretrained word vectors\n","# get a dict of tokens (key) and their pretrained word vectors (value)\n","# pretrained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n","dim = 0\n","word_vecs= {}\n","# open pretrained word vector file\n","with open('.\\\\raw_data\\\\cna.cbow.cwe_p.tar_g.512d.0.txt', encoding='utf-8') as f:\n","    for line in f:\n","        tokens = line.strip().split()\n","\n","        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n","        if len(tokens) == 2:\n","            dim = int(tokens[1])\n","            continue\n","    \n","        word = tokens[0]\n","        vec = np.array([ float(t) for t in tokens[1:] ])\n","        word_vecs[word] = vec"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print('vocabulary_size: ',len(word_vecs),' word_vector_dim: ',vec.shape)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# simply load data without splitting\n","def Dataset(data_path):\n","    with open(data_path, 'r', encoding='utf-8') as f:\n","        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n","    data_list, data_list_tmp = list(), list()\n","    article_id_list=list()\n","    idx=0\n","    for row in data:\n","        data_tuple = tuple()\n","        if row == '\\n':\n","            article_id_list.append(idx)\n","            idx+=1\n","            data_list.append(data_list_tmp)\n","            data_list_tmp = []\n","        else:\n","            row = row.strip('\\n').split(' ')\n","            data_tuple = (row[0], row[1])\n","            data_list_tmp.append(data_tuple)\n","    if len(data_list_tmp) != 0:\n","        data_list.append(data_list_tmp)\n","\n","    return data_list"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# open the pos tag file\n","df_POSTag = pd.read_csv('.\\\\processed_data\\\\train2_POSTag.txt')\n","df_POSTag.head()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["print(len(df_POSTag))\n","print(len(df_POSTag['POS'].unique()))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# look up the POSTag txt\n","# encode the word\n","def POSTagEncode(data_list, POSTag_csv='.\\\\processed_data\\\\train2_POSTag.txt'):\n","    \n","    \n","    POSTag_list = list()\n","\n","    df = pd.read_csv(POSTag_csv)\n","    df = df[df['entity_text'] != ' ']\n","    POSTag_label = list(df['POS'].unique())\n","\n","    for idx_list in range(len(data_list)):\n","        df_temp = df[df['article_id']==idx_list]\n","        POS_temp  = list(df_temp['POS'])\n","        print(idx_list,'\\'',str(len(data_list[idx_list])-len(POS_temp)))\n","        POSTag_list_temp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            word_POSTag_temp = list()\n","            for POSTag_label_code in POSTag_label:\n","                if POS_temp[idx_tuple] == POSTag_label_code:\n","                    word_POSTag_temp.append(1)\n","                else:\n","                    word_POSTag_temp.append(0)\n","                    \n","            POSTag_list_temp.append(word_POSTag_temp)\n","        POSTag_list.append(POSTag_list_temp)\n","\n","    return POSTag_list"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# look up word vectors\n","# turn each word into its pretrained word vector\n","# return a list of word vectors corresponding to each token in train.data\n","def Word2Vector(data_list, embedding_dict):\n","    embedding_list = list()\n","\n","    # No Match Word (unknown word) Vector in Embedding\n","    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n","\n","    for idx_list in range(len(data_list)):\n","        embedding_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            key = data_list[idx_list][idx_tuple][0] # token\n","\n","            if key in embedding_dict:\n","                value = embedding_dict[key]\n","            else:\n","                value = unk_vector\n","            embedding_list_tmp.append(value)\n","        embedding_list.append(embedding_list_tmp)\n","    \n","    return embedding_list"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# input features: pretrained word vectors of each token\n","# return a list of feature dicts, each feature dict corresponding to each token\n","def Feature(embed_list, p):\n","    \n","    df = pd.read_csv('.\\\\processed_data\\\\train2_POSTag.txt')\n","    POS_unique_list = list(df['POS'].unique())\n","    # alphabet_list = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","    # for alpha in alphabet_list:\n","    #     POS_unique_list.append(alpha)\n","    POS_unique_list.append('Start') # 1 if no last word\n","    \n","    feature_list = list()\n","    \n","    # feature of w2d (original)\n","    for idx_list in range(len(embed_list)):\n","        feature_list_tmp = list()\n","        for idx_tuple in range(len(embed_list[idx_list])):\n","            \n","            feature_dict = dict()\n","            \n","            # feature of word's POSTag(56+56+1)\n","            feature_dict['Start'] = 0\n","            for idx_POS in range(len(POS_unique_list)-1): # exclude Start\n","                feature_dict[POS_unique_list[idx_POS]] = p[idx_list][idx_tuple][idx_POS]\n","                if idx_tuple != 0:\n","                    feature_dict['last_' + POS_unique_list[idx_POS]] = p[idx_list][idx_tuple-1][idx_POS]\n","                else:\n","                    feature_dict['Start'] = 1\n","\n","            #-----------------\n","            # feature of word's vector(512)\n","            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n","                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n","\n","            feature_list_tmp.append(feature_dict)\n","        feature_list.append(feature_list_tmp)\n","        print(idx_list+1, '\\\\', len(embed_list)+1, ', # of token:', len(embed_list[idx_list]))\n","        \n","    return feature_list"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# get the labels of each tokens in train.data\n","# return a list of lists of labels\n","def Preprocess(data_list):\n","    label_list = list()\n","    for idx_list in range(len(data_list)):\n","        label_list_tmp = list()\n","        for idx_tuple in range(len(data_list[idx_list])):\n","            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n","        label_list.append(label_list_tmp)\n","    return label_list"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["data_list = Dataset('.\\\\data\\\\train2_sample.data')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["p = POSTagEncode(data_list)\n","embed_list = Word2Vector(data_list, word_vecs)\n","X = Feature(embed_list, p)\n","y = Preprocess(data_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# release resources\n","if 'embed_list' in globals():\n","    del embed_list\n","if 'p' in globals():\n","    del p\n","if 'data_list' in globals():\n","    del data_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import joblib\n","\n","labels, crf = CRF(X, y)\n","joblib.dump(crf, 'crf_150iter')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def loadInputFile(file_path):\n","    with open(file_path, 'r', encoding='utf8') as f:\n","        dev_set = list()\n","        \n","        while(True):\n","            article_id = f.readline()\n","            if 'article_id:' not in article_id:\n","                break\n","            else:\n","                dev_set.append(f.readline())\n","            f.readline()\n","            f.readline()\n","            f.readline()\n","            \n","    return dev_set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dev_set = loadInputFile('raw_data\\development_2.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["p = POSTagEncode(dev_set, '.\\\\processed_data\\\\dev2_POSTag.txt')\n","embed_list = Word2Vector(dev_set, word_vecs)\n","X = Feature(embed_list, p)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_pred = crf.predict(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["print(len(y_pred))\n","print(len(y_pred[0]))\n","print(y_pred[0][0])\n","print(y_pred[0][0][0])\n","print(len(dev_set[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["i = 0\n","for dev_id in range(len(y_pred)):\n","    for pred_id in range(len(y_pred[dev_id])):\n","        if y_pred[dev_id][pred_id][0] == 'B':\n","            i = i+1\n","print(i)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n","for dev_id in range(len(y_pred)):\n","    pos=0\n","    start_pos=None\n","    end_pos=None\n","    entity_text=None\n","    entity_type=None\n","    for pred_id in range(len(y_pred[dev_id])):\n","        if y_pred[dev_id][pred_id][0]=='B':\n","            start_pos=pos\n","            entity_type=y_pred[dev_id][pred_id][2:]\n","        elif start_pos is not None and y_pred[dev_id][pred_id][0]=='I'and pred_id<len(y_pred[dev_id])-1 and y_pred[dev_id][pred_id+1][0]=='O':\n","            end_pos=pos\n","            entity_text=''.join([dev_set[dev_id][position] for position in range(start_pos,end_pos+1)])\n","            line=str(dev_id)+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n","            output+=line+'\\n'\n","        pos+=1     "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_path='output2.tsv'\n","with open(output_path,'w',encoding='utf-8') as f:\n","    f.write(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(output)"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.6.7 64-bit","metadata":{"interpreter":{"hash":"bc5325771484cd741460cbbcf20a153b1ef45b389ce85e60a186ce6bd8aea41f"}}},"language_info":{"name":"python","version":"3.6.7-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}